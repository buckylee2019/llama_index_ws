{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "!export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128'"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting peft\n  Downloading peft-0.2.0-py3-none-any.whl (40 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from peft) (6.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from peft) (5.9.0)\nCollecting torch>=1.13.0\n  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting accelerate\n  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from peft) (4.17.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from peft) (1.23.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.0.9)\nRequirement already satisfied: jinja2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.0.3)\nCollecting nvidia-nccl-cu11==2.14.3\n  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting triton==2.0.0\n  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.6.0)\nCollecting nvidia-cuda-runtime-cu11==11.7.99\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.13.0->peft) (4.3.0)\nCollecting nvidia-cusolver-cu11==11.4.0.1\n  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting sympy\n  Downloading sympy-1.11.1-py3-none-any.whl (6.5 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: networkx in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.8.4)\nCollecting nvidia-nvtx-cu11==11.7.91\n  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft) (65.6.3)\nRequirement already satisfied: wheel in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft) (0.38.4)\nCollecting cmake\n  Downloading cmake-3.26.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting lit\n  Downloading lit-16.0.0.tar.gz (144 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from transformers->peft) (0.10.1)\nRequirement already satisfied: requests in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from transformers->peft) (2.28.1)\nRequirement already satisfied: sacremoses in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from transformers->peft) (0.0.43)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from transformers->peft) (4.64.0)\nRequirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from transformers->peft) (0.11.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from transformers->peft) (2022.7.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests->transformers->peft) (2022.12.7)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests->transformers->peft) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests->transformers->peft) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests->transformers->peft) (1.26.11)\nRequirement already satisfied: six in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from sacremoses->transformers->peft) (1.16.0)\nRequirement already satisfied: click in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from sacremoses->transformers->peft) (8.0.4)\nRequirement already satisfied: joblib in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from sacremoses->transformers->peft) (1.1.1)\nCollecting mpmath>=0.19\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: lit\n  Building wheel for lit (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lit: filename=lit-16.0.0-py3-none-any.whl size=93598 sha256=26b83d50def8c39c944e82b8cd8f2704935242914875896c240cbb93bfdb61a3\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/a3/f5/8f/b11227e816563ac08fce423c6e617f05f5fe1a18d9bcfb2375\nSuccessfully built lit\nInstalling collected packages: mpmath, lit, cmake, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, accelerate, peft\n  Attempting uninstall: torch\n    Found existing installation: torch 1.12.1\n    Uninstalling torch-1.12.1:\n      Successfully uninstalled torch-1.12.1\nSuccessfully installed accelerate-0.18.0 cmake-3.26.1 lit-16.0.0 mpmath-1.3.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 peft-0.2.0 sympy-1.11.1 torch-2.0.0 triton-2.0.0\nCollecting gradio\n  Downloading gradio-3.24.1-py3-none-any.whl (15.7 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from gradio) (3.8.1)\nRequirement already satisfied: matplotlib in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from gradio) (3.5.2)\nCollecting fastapi\n  Downloading fastapi-0.95.0-py3-none-any.whl (57 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting gradio-client>=0.0.5\n  Downloading gradio_client-0.0.7-py3-none-any.whl (14 kB)\nCollecting aiofiles\n  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\nCollecting websockets>=10.0\n  Downloading websockets-11.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m129.7/129.7 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from gradio) (6.0)\nCollecting httpx\n  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from gradio) (1.4.3)\nCollecting altair>=4.2.0\n  Downloading altair-4.2.2-py3-none-any.whl (813 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m813.6/813.6 kB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting markdown-it-py[linkify]>=2.0.0\n  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: markupsafe in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from gradio) (2.1.1)\nCollecting mdit-py-plugins<=0.3.3\n  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pillow in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from gradio) (9.3.0)\nCollecting python-multipart\n  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting huggingface-hub>=0.13.0\n  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m199.8/199.8 kB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from gradio) (2.28.1)\nCollecting semantic-version\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nCollecting ffmpy\n  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from gradio) (3.0.3)\nRequirement already satisfied: numpy in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from gradio) (1.23.1)\nCollecting pydantic\n  Downloading pydantic-1.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting uvicorn\n  Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pydub\n  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\nRequirement already satisfied: typing-extensions in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from gradio) (4.3.0)\nCollecting orjson\n  Downloading orjson-3.8.9-cp310-cp310-manylinux_2_28_x86_64.whl (144 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m144.1/144.1 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: entrypoints in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from altair>=4.2.0->gradio) (0.4)\nRequirement already satisfied: toolz in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from altair>=4.2.0->gradio) (0.11.2)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from altair>=4.2.0->gradio) (4.4.0)\nRequirement already satisfied: fsspec in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from gradio-client>=0.0.5->gradio) (2021.10.1)\nRequirement already satisfied: packaging in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from gradio-client>=0.0.5->gradio) (21.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from huggingface-hub>=0.13.0->gradio) (4.64.0)\nRequirement already satisfied: filelock in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from huggingface-hub>=0.13.0->gradio) (3.6.0)\nCollecting mdurl~=0.1\n  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nCollecting linkify-it-py<3,>=1\n  Downloading linkify_it_py-2.0.0-py3-none-any.whl (19 kB)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from pandas->gradio) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from pandas->gradio) (2022.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->gradio) (4.0.1)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->gradio) (1.8.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->gradio) (1.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->gradio) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->gradio) (21.4.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->gradio) (5.2.0)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->gradio) (2.0.4)\nCollecting starlette<0.27.0,>=0.26.1\n  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sniffio\n  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\nCollecting rfc3986[idna2008]<2,>=1.3\n  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\nRequirement already satisfied: certifi in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from httpx->gradio) (2022.12.7)\nCollecting httpcore<0.17.0,>=0.15.0\n  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m69.6/69.6 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from matplotlib->gradio) (4.25.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from matplotlib->gradio) (3.0.9)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from matplotlib->gradio) (1.4.2)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from matplotlib->gradio) (0.11.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests->gradio) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests->gradio) (1.26.11)\nCollecting h11>=0.8\n  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: click>=7.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from uvicorn->gradio) (8.0.4)\nCollecting anyio<5.0,>=3.0\n  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m80.6/80.6 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.18.0)\nCollecting uc-micro-py\n  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->gradio) (1.16.0)\nBuilding wheels for collected packages: ffmpy\n  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4707 sha256=ebfb1c7c1d020105e69cb6d5d2fb214cd7dc0cc8cabf67147726522b81591b1c\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/0c/c2/0e/3b9c6845c6a4e35beb90910cc70d9ac9ab5d47402bd62af0df\nSuccessfully built ffmpy\nInstalling collected packages: rfc3986, pydub, ffmpy, websockets, uc-micro-py, sniffio, semantic-version, python-multipart, pydantic, orjson, mdurl, h11, aiofiles, uvicorn, markdown-it-py, linkify-it-py, huggingface-hub, anyio, starlette, mdit-py-plugins, httpcore, gradio-client, altair, httpx, fastapi, gradio\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.10.1\n    Uninstalling huggingface-hub-0.10.1:\n      Successfully uninstalled huggingface-hub-0.10.1\nSuccessfully installed aiofiles-23.1.0 altair-4.2.2 anyio-3.6.2 fastapi-0.95.0 ffmpy-0.3.0 gradio-3.24.1 gradio-client-0.0.7 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 huggingface-hub-0.13.3 linkify-it-py-2.0.0 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdurl-0.1.2 orjson-3.8.9 pydantic-1.10.7 pydub-0.25.1 python-multipart-0.0.6 rfc3986-1.5.0 semantic-version-2.10.0 sniffio-1.3.0 starlette-0.26.1 uc-micro-py-1.0.1 uvicorn-0.21.1 websockets-11.0\nCollecting fire\n  Downloading fire-0.5.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from fire) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from fire) (1.1.0)\nBuilding wheels for collected packages: fire\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116951 sha256=ddd9ab0d411d749cda5ef12fd6ecead7f06e7ef65105165e4efdd5c79c4678c4\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\nSuccessfully built fire\nInstalling collected packages: fire\nSuccessfully installed fire-0.5.0\n"
                }
            ],
            "source": "!pip install peft\n!pip install gradio\n!pip install fire"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting llama-index\n  Downloading llama_index-0.5.8.tar.gz (165 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m165.4/165.4 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting dataclasses_json\n  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\nCollecting langchain\n  Downloading langchain-0.0.132-py3-none-any.whl (489 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m489.1/489.1 kB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from llama-index) (1.23.1)\nCollecting tenacity<9.0.0,>=8.2.0\n  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\nCollecting openai>=0.26.4\n  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from llama-index) (1.4.3)\nCollecting tiktoken\n  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from openai>=0.26.4->llama-index) (3.8.1)\nRequirement already satisfied: requests>=2.20 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from openai>=0.26.4->llama-index) (2.28.1)\nRequirement already satisfied: tqdm in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from openai>=0.26.4->llama-index) (4.64.0)\nCollecting typing-inspect>=0.4.0\n  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\nCollecting marshmallow-enum<2.0.0,>=1.5.1\n  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\nCollecting marshmallow<4.0.0,>=3.3.0\n  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from langchain->llama-index) (6.0)\nRequirement already satisfied: pydantic<2,>=1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from langchain->llama-index) (1.10.7)\nCollecting aiohttp\n  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: SQLAlchemy<2,>=1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from langchain->llama-index) (1.4.39)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from pandas->llama-index) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from pandas->llama-index) (2022.1)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from tiktoken->llama-index) (2022.7.9)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->openai>=0.26.4->llama-index) (1.8.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->openai>=0.26.4->llama-index) (5.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->openai>=0.26.4->llama-index) (1.2.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->openai>=0.26.4->llama-index) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->openai>=0.26.4->llama-index) (21.4.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->openai>=0.26.4->llama-index) (2.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->openai>=0.26.4->llama-index) (4.0.1)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses_json->llama-index) (21.3)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from pydantic<2,>=1->langchain->llama-index) (4.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests>=2.20->openai>=0.26.4->llama-index) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests>=2.20->openai>=0.26.4->llama-index) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests>=2.20->openai>=0.26.4->llama-index) (1.26.11)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from SQLAlchemy<2,>=1->langchain->llama-index) (1.1.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses_json->llama-index) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses_json->llama-index) (3.0.9)\nBuilding wheels for collected packages: llama-index\n  Building wheel for llama-index (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for llama-index: filename=llama_index-0.5.8-py3-none-any.whl size=248451 sha256=b559f221bec6f93db706c2597656637c2c6c96aeb1a8b9e0d8df8e892e05e71f\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/05/a8/f2/0f51d342abd831ba051d15c3dddd4c1c3de0516e6e01f7c3e0\nSuccessfully built llama-index\nInstalling collected packages: typing-inspect, tenacity, tiktoken, marshmallow, aiohttp, openai, marshmallow-enum, dataclasses_json, langchain, llama-index\n  Attempting uninstall: tenacity\n    Found existing installation: tenacity 8.0.1\n    Uninstalling tenacity-8.0.1:\n      Successfully uninstalled tenacity-8.0.1\n  Attempting uninstall: aiohttp\n    Found existing installation: aiohttp 3.8.1\n    Uninstalling aiohttp-3.8.1:\n      Successfully uninstalled aiohttp-3.8.1\nSuccessfully installed aiohttp-3.8.4 dataclasses_json-0.5.7 langchain-0.0.132 llama-index-0.5.8 marshmallow-3.19.0 marshmallow-enum-1.5.1 openai-0.27.4 tenacity-8.2.2 tiktoken-0.3.3 typing-inspect-0.8.0\nNote: you may need to restart the kernel to use updated packages.\n"
                }
            ],
            "source": "pip install llama-index"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Note: you may need to restart the kernel to use updated packages.\n"
                }
            ],
            "source": "pip -q install git+https://github.com/huggingface/transformers"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: sentencepiece in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (0.1.96)\nRequirement already satisfied: accelerate in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (0.18.0)\nRequirement already satisfied: psutil in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from accelerate) (5.9.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: pyyaml in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from accelerate) (6.0)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from accelerate) (1.23.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (11.7.101)\nRequirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (2.14.3)\nRequirement already satisfied: triton==2.0.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (2.0.0)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (11.10.3.66)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (11.7.4.91)\nRequirement already satisfied: sympy in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (1.11.1)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (11.7.99)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (10.9.0.58)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (11.4.0.1)\nRequirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (10.2.10.91)\nRequirement already satisfied: typing-extensions in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (4.3.0)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (8.5.0.96)\nRequirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (11.7.91)\nRequirement already satisfied: networkx in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (2.8.4)\nRequirement already satisfied: jinja2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (3.0.3)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (11.7.99)\nRequirement already satisfied: filelock in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (3.6.0)\nRequirement already satisfied: wheel in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate) (0.38.4)\nRequirement already satisfied: setuptools in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate) (65.6.3)\nRequirement already satisfied: lit in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (16.0.0)\nRequirement already satisfied: cmake in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (3.26.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->accelerate) (2.1.1)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from sympy->torch>=1.4.0->accelerate) (1.3.0)\nCollecting bitsandbytes==0.35.0\n  Downloading bitsandbytes-0.35.0-py3-none-any.whl (62.5 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.35.0\n"
                }
            ],
            "source": "!pip install sentencepiece\n!pip install accelerate\n!pip install bitsandbytes==0.35.0"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\nFor effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n================================================================================\nCUDA SETUP: CUDA runtime path found: /opt/conda/envs/Python-3.10-CUDA/lib/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 7.0\nCUDA SETUP: Detected CUDA version 114\nCUDA SETUP: Loading binary /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda114_nocublaslt.so...\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \nThe class this function is called from is 'LlamaTokenizer'.\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "cpu\n"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d3da8960602e499ba60ddb0dfa98ed96",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "The model 'OptimizedModule' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\nINFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\nINFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
                }
            ],
            "source": "import torch\nfrom langchain.llms.base import LLM\nfrom llama_index import SimpleDirectoryReader, LangchainEmbedding, GPTListIndex, PromptHelper\nfrom llama_index import LLMPredictor, ServiceContext\nfrom typing import Optional, List, Mapping, Any\nfrom transformers import LlamaTokenizer, LlamaForCausalLM, pipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom langchain.llms import HuggingFacePipeline\nfrom accelerate import init_empty_weights\nfrom llama_index import download_loader\nimport sys\nfrom peft import PeftModel\n\ntorch.cuda.empty_cache()\n# define prompt helper\n# set maximum input size\nmax_input_size = 2048\n# set number of output tokens\nnum_output = 256\n# set maximum chunk overlap\nmax_chunk_overlap = 20\nprompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n# device_map = {\n#     \"transformer.word_embeddings\": 0,\n#     \"transformer.word_embeddings_layernorm\": 0,\n#     \"lm_head\": 0,\n#     \"transformer.h\":0 ,\n#     \"transformer.ln_f\": 0,\n#     \"model\":\"cpu\",\n# }\n\n\n# quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n# class CustomLLM(LLM):\n#     checkpoint = \"decapoda-research/llama-7b-hf\"\n#     tokenizer = LlamaTokenizer.from_pretrained(checkpoint)\n#     max_memory_mapping = {0: \"14GB\", 1: \"10GB\"}\n#     base_model = LlamaForCausalLM.from_pretrained(\n#         checkpoint,\n#         device_map=device_map,\n#         load_in_8bit=True,\n#         torch_dtype='auto',\n#         quantization_config=quantization_config,\n#     )\n\n#     self.pipeline = pipeline(\n#         \"text-generation\",\n#         model=base_model,\n#         tokenizer=tokenizer,\n#         max_length=256,\n#         temperature=0.6,\n#         top_p=0.95,\n#         repetition_penalty=1.2\n#     )\n#     def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n#         prompt_length = len(prompt)\n#         response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n\n#         # only return newly generated tokens\n#         return response[prompt_length:]\n\n#     @property\n#     def _identifying_params(self) -> Mapping[str, Any]:\n#         return {\"name_of_model\": self.model_name}\n\n#     @property\n#     def _llm_type(self) -> str:\n#         return \"custom\"\ndevice = \"cpu\"\nload_8bit = True\nbase_model = \"decapoda-research/llama-7b-hf\"\nlora_weights = \"tloen/alpaca-lora-7b\"\n# The prompt template to use, will default to alpaca.\nprompt_template = \"\"\n# Allows to listen on all interfaces by providing '0.\n\nshare_gradio = False\n\n\nbase_model = base_model or os.environ.get(\"BASE_MODEL\", \"\")\nassert (\n    base_model\n), \"Please specify a --base_model, e.g. --base_model='decapoda-research/llama-7b-hf'\"\n\ntokenizer = LlamaTokenizer.from_pretrained(base_model)\nprint(device)\nif device == \"cuda\":\n    model = LlamaForCausalLM.from_pretrained(\n        base_model,\n        load_in_8bit=load_8bit,\n        torch_dtype=torch.float16,\n        device_map={'': 0},\n    )\n    model = PeftModel.from_pretrained(\n        model,\n        lora_weights,\n        torch_dtype=torch.float16,\n    )\nelif device == \"mps\":\n    model = LlamaForCausalLM.from_pretrained(\n        base_model,\n        device_map={\"\": device},\n        torch_dtype=torch.float16,\n    )\n    model = PeftModel.from_pretrained(\n        model,\n        lora_weights,\n        device_map={\"\": device},\n        torch_dtype=torch.float16,\n    )\nelse:\n    model = LlamaForCausalLM.from_pretrained(\n        base_model, device_map={\"\": device}, low_cpu_mem_usage=True\n    )\n    model = PeftModel.from_pretrained(\n        model,\n        lora_weights,\n        device_map={\"\": device},\n    )\n\n# unwind broken decapoda-research config\nmodel.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\nmodel.config.bos_token_id = 1\nmodel.config.eos_token_id = 2\n\nif not load_8bit:\n    model.half()  # seems to fix bugs for some users.\n\nmodel.eval()\nif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n    model = torch.compile(model)\n\n# define prompt helper\n# set maximum input size\nmax_input_size = 2048\n# set number of output tokens\nnum_output = 200\n# set maximum chunk overlap\nmax_chunk_overlap = 20\nclass CustomLLM(LLM):\n    model_name = 'decapoda-research/llama-7b-hf'\n    pipeline = pipeline(\n        task='text-generation',\n        model=model,\n        tokenizer=tokenizer,\n        device=\"cpu\",\n    )\n    temperature = 0.4\n    top_p = 0.75\n    top_k = 30\n    num_beams = 1\n    max_new_tokens = 128\n    repetition_penalty = 1.3\n\n    def set_params(self, temperature=0.1, top_p=0.75, top_k=40, num_beams=4, max_new_tokens=128):\n        super().__init__()\n        self.temperature = temperature\n        self.top_p = top_p\n        self.top_k = top_k\n        self.num_beams = num_beams\n        self.max_new_tokens = max_new_tokens\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        prompt_len = len(prompt)\n        response = self.pipeline(\n            prompt,\n            max_new_tokens=self.max_new_tokens,\n            repetition_penalty=self.repetition_penalty,\n            temperature=self.temperature,\n            top_p=self.top_p,\n            top_k=self.top_k,\n            num_beams=self.num_beams,\n            stop_sequence=[\".\"]\n        )[0][\"generated_text\"]\n        return response[prompt_len:]\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        return {\"name_of_model\": self.model_name}\n\n    @property\n    def _llm_type(self) -> str:\n        return \"custom\"\n\n# define our LLM\nlocal_llm = CustomLLM()\nllm_predictor = LLMPredictor(llm=local_llm)\n\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n\n# Load the your data\n\nWikipediaReader = download_loader(\"WikipediaReader\")\n\nloader = WikipediaReader()\ndocuments = loader.load_data(pages=['Berlin', 'Rome', 'Tokyo', 'Canberra', 'Santiago'])\n\nindex = GPTListIndex.from_documents(documents, service_context=service_context)\n\n# Query and print response\n"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "INFO:root:___________Promt:____________\nINFO:root:Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\nWhat do you think of Facebook's LLaMa?\nAnswer:\nINFO:root:_________Normal model____________\n/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n  warnings.warn(\nINFO:root: I am excited to see what new features and capabilities this platform will bring! Home \u00bb News & Events \u00bb 2018 \u00bb March \u00bb Firefighters tackle blaze at house in Birmingham city centre\nFirefighters have been called out after reports of smoke coming from a property on Newhall Street, near Five Ways Island.\nINFO:root:_________After Documents___________\nINFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 76750 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 0 tokens\nINFO:root: #include \"stdafx.\nINFO:root:______________________________\nINFO:root:___________Promt:____________\nINFO:root:Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\nHow many people lives in Martos?\nAnswer:\nINFO:root:_________Normal model____________\nINFO:root: 10,389 Home \u00bb News & Events \u00bb News Releases \u00bb New Study Shows How to Improve Health Care for People with Disabilities and Chronic Illnesses\nNew Study Shows How to Improve Health Care for People with Disabilities and Chronic Illnesses\nWashington D.\nINFO:root:_________After Documents___________\nINFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 77945 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 0 tokens\nINFO:root: Tags: c#, asp.\nINFO:root:______________________________\nINFO:root:___________Promt:____________\nINFO:root:Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\nWhat is the capital of England?\nAnswer:\nINFO:root:_________Normal model____________\nINFO:root: London Home \u00bb News & Events \u00bb Blog Archive \u00bb 2017 \u00bb September (Page 3)\nThe Importance Of A Good Night\u2019s Sleep For Children With Autism Spectrum Disorder \u2013 By Dr.\nINFO:root:_________After Documents___________\nINFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 76736 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 0 tokens\nINFO:root: Tags: javascript, jquery, html, css\n\nQuestion: jQuery - how can i make this code work properly without errors?\n\n\\begin{code}\n$(document).ready(function(){\n    var $div = $('');\n        $(\"body\").append($div);\n});\n\\end{code}\n\nComment: What error are you getting? Please provide details about your problem so we may help you out!!!\n\nComment: You need to add `var` before `$div`. Also it will be easier to read when using camelCase instead of snake_case like `$\nINFO:root:______________________________\nINFO:root:___________Promt:____________\nINFO:root:Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\nWhat are alpacas? and how are they different from llamas?\nAnswer:\nINFO:root:_________Normal model____________\nINFO:root: Alpaca's are small, domesticated animals native to South America which belong to the camelid family along with other species such as llama\u2019s, vicu\u00f1as, guanacoes etc.. They have long necks, short legs and soft woolly coats in various shades of brown or white.\nINFO:root:_________After Documents___________\n"
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m evaluate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow many people lives in Martos?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m evaluate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of England?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat are alpacas? and how are they different from llamas?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m evaluate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhow much is 213769*121239?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(instruction)\u001b[0m\n\u001b[1;32m     24\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(llm_chain\u001b[38;5;241m.\u001b[39mrun(instruction));\n\u001b[1;32m     25\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_________After Documents___________\u001b[39m\u001b[38;5;124m\"\u001b[39m);\n\u001b[0;32m---> 26\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m)\u001b[49m);\n\u001b[1;32m     27\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m______________________________\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/llama_index/indices/base.py:244\u001b[0m, in \u001b[0;36mBaseGPTIndex.query\u001b[0;34m(self, query_str, mode, query_transform, use_async, **query_kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m query_config \u001b[38;5;241m=\u001b[39m QueryConfig(\n\u001b[1;32m    231\u001b[0m     index_struct_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct\u001b[38;5;241m.\u001b[39mget_type(),\n\u001b[1;32m    232\u001b[0m     query_mode\u001b[38;5;241m=\u001b[39mmode_enum,\n\u001b[1;32m    233\u001b[0m     query_kwargs\u001b[38;5;241m=\u001b[39mquery_kwargs,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m query_runner \u001b[38;5;241m=\u001b[39m QueryRunner(\n\u001b[1;32m    236\u001b[0m     index_struct\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct,\n\u001b[1;32m    237\u001b[0m     service_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m     use_async\u001b[38;5;241m=\u001b[39muse_async,\n\u001b[1;32m    243\u001b[0m )\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/llama_index/indices/query/query_runner.py:342\u001b[0m, in \u001b[0;36mQueryRunner.query\u001b[0;34m(self, query_str_or_bundle, index_id, level)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m\"\"\"Run query.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03mNOTE: Relies on mutual recursion between\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m    composable graph.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m query_combiner, query_bundle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_query_objects(\n\u001b[1;32m    340\u001b[0m     query_str_or_bundle, index_id\u001b[38;5;241m=\u001b[39mindex_id\n\u001b[1;32m    341\u001b[0m )\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery_combiner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/llama_index/indices/query/query_combiner/base.py:66\u001b[0m, in \u001b[0;36mSingleQueryCombiner.run\u001b[0;34m(self, query_bundle, level)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"Run query combiner.\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m updated_query_bundle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_update(query_bundle)\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_transformed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdated_query_bundle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/llama_index/indices/query/query_runner.py:202\u001b[0m, in \u001b[0;36mQueryRunner.query_transformed\u001b[0;34m(self, query_bundle, index_struct, level)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/llama_index/token_counter/token_counter.py:78\u001b[0m, in \u001b[0;36mllm_token_counter.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_llm_predict\u001b[39m(_self: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m wrapper_logic(_self):\n\u001b[0;32m---> 78\u001b[0m         f_return_val \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f_return_val\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/llama_index/indices/query/base.py:396\u001b[0m, in \u001b[0;36mBaseGPTIndexQuery.query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;124;03m\"\"\"Answer a query.\"\"\"\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m# TODO: support include summary\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/llama_index/indices/query/base.py:383\u001b[0m, in \u001b[0;36mBaseGPTIndexQuery._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# TODO: remove _query and just use query\u001b[39;00m\n\u001b[1;32m    382\u001b[0m nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve(query_bundle)\n\u001b[0;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/llama_index/indices/query/base.py:340\u001b[0m, in \u001b[0;36mBaseGPTIndexQuery.synthesize\u001b[0;34m(self, query_bundle, nodes, additional_source_nodes)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_response_builder(\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_builder,\n\u001b[1;32m    334\u001b[0m     query_bundle,\n\u001b[1;32m    335\u001b[0m     nodes,\n\u001b[1;32m    336\u001b[0m     additional_source_nodes\u001b[38;5;241m=\u001b[39madditional_source_nodes,\n\u001b[1;32m    337\u001b[0m )\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_mode \u001b[38;5;241m!=\u001b[39m ResponseMode\u001b[38;5;241m.\u001b[39mNO_TEXT:\n\u001b[0;32m--> 340\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_give_response_for_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse_builder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/llama_index/indices/query/base.py:223\u001b[0m, in \u001b[0;36mBaseGPTIndexQuery._give_response_for_nodes\u001b[0;34m(self, response_builder, query_str)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_give_response_for_nodes\u001b[39m(\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m, response_builder: ResponseBuilder, query_str: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m    221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TEXT_TYPE:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;124;03m\"\"\"Give response for nodes.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mresponse_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_response_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_response_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/llama_index/indices/response/builder.py:387\u001b[0m, in \u001b[0;36mResponseBuilder.get_response\u001b[0;34m(self, query_str, prev_response, mode, **response_kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m\"\"\"Get response.\"\"\"\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ResponseMode\u001b[38;5;241m.\u001b[39mDEFAULT:\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_response_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ResponseMode\u001b[38;5;241m.\u001b[39mCOMPACT:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_response_compact(query_str, prev_response)\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/llama_index/indices/response/builder.py:242\u001b[0m, in \u001b[0;36mResponseBuilder._get_response_default\u001b[0;34m(self, query_str, prev_response)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_response_default\u001b[39m(\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m, query_str: \u001b[38;5;28mstr\u001b[39m, prev_response: Optional[\u001b[38;5;28mstr\u001b[39m]\n\u001b[1;32m    241\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TEXT_TYPE:\n\u001b[0;32m--> 242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response_over_chunks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_response\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/llama_index/indices/response/builder.py:229\u001b[0m, in \u001b[0;36mResponseBuilder.get_response_over_chunks\u001b[0;34m(self, query_str, text_chunks, prev_response)\u001b[0m\n\u001b[1;32m    224\u001b[0m             response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgive_response_single(\n\u001b[1;32m    225\u001b[0m                 query_str,\n\u001b[1;32m    226\u001b[0m                 text_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m    227\u001b[0m             )\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefine_response_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprev_response_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_chunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     prev_response_obj \u001b[38;5;241m=\u001b[39m response\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mstr\u001b[39m):\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/llama_index/indices/response/builder.py:142\u001b[0m, in \u001b[0;36mResponseBuilder.refine_response_single\u001b[0;34m(self, response, query_str, text_chunk)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cur_text_chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[1;32m    139\u001b[0m         (\n\u001b[1;32m    140\u001b[0m             response,\n\u001b[1;32m    141\u001b[0m             formatted_prompt,\n\u001b[0;32m--> 142\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_service_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrefine_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext_msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_text_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m         response, formatted_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mllm_predictor\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m    148\u001b[0m             refine_template,\n\u001b[1;32m    149\u001b[0m             context_msg\u001b[38;5;241m=\u001b[39mcur_text_chunk,\n\u001b[1;32m    150\u001b[0m         )\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/llama_index/llm_predictor/base.py:223\u001b[0m, in \u001b[0;36mLLMPredictor.predict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m\"\"\"Predict the answer to a query.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m \n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m formatted_prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mformat(llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_args)\n\u001b[0;32m--> 223\u001b[0m llm_prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprompt_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(llm_prediction)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# We assume that the value of formatted_prompt is exactly the thing\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# eventually sent to OpenAI, or whatever LLM downstream\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/llama_index/llm_predictor/base.py:197\u001b[0m, in \u001b[0;36mLLMPredictor._predict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    195\u001b[0m full_prompt_args \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mget_full_format_args(prompt_args)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_on_throttling:\n\u001b[0;32m--> 197\u001b[0m     llm_prediction \u001b[38;5;241m=\u001b[39m \u001b[43mretry_on_exceptions_with_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_prompt_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m            \u001b[49m\u001b[43mErrorToRetry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRateLimitError\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m            \u001b[49m\u001b[43mErrorToRetry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mServiceUnavailableError\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m            \u001b[49m\u001b[43mErrorToRetry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTryAgain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m            \u001b[49m\u001b[43mErrorToRetry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m                \u001b[49m\u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPIConnectionError\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshould_retry\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     llm_prediction \u001b[38;5;241m=\u001b[39m llm_chain\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfull_prompt_args)\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/llama_index/utils.py:162\u001b[0m, in \u001b[0;36mretry_on_exceptions_with_backoff\u001b[0;34m(lambda_fn, errors_to_retry, max_tries, min_backoff_secs, max_backoff_secs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlambda_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exception_class_tuples \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    164\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/llama_index/llm_predictor/base.py:198\u001b[0m, in \u001b[0;36mLLMPredictor._predict.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m full_prompt_args \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mget_full_format_args(prompt_args)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_on_throttling:\n\u001b[1;32m    197\u001b[0m     llm_prediction \u001b[38;5;241m=\u001b[39m retry_on_exceptions_with_backoff(\n\u001b[0;32m--> 198\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_prompt_args\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    199\u001b[0m         [\n\u001b[1;32m    200\u001b[0m             ErrorToRetry(openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mRateLimitError),\n\u001b[1;32m    201\u001b[0m             ErrorToRetry(openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mServiceUnavailableError),\n\u001b[1;32m    202\u001b[0m             ErrorToRetry(openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mTryAgain),\n\u001b[1;32m    203\u001b[0m             ErrorToRetry(\n\u001b[1;32m    204\u001b[0m                 openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mAPIConnectionError, \u001b[38;5;28;01mlambda\u001b[39;00m e: e\u001b[38;5;241m.\u001b[39mshould_retry\n\u001b[1;32m    205\u001b[0m             ),\n\u001b[1;32m    206\u001b[0m         ],\n\u001b[1;32m    207\u001b[0m     )\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     llm_prediction \u001b[38;5;241m=\u001b[39m llm_chain\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfull_prompt_args)\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/langchain/chains/llm.py:151\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/langchain/chains/base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/langchain/chains/base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    108\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    109\u001b[0m     inputs,\n\u001b[1;32m    110\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/langchain/chains/llm.py:57\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/langchain/chains/llm.py:118\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_list: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;124;03m\"\"\"Utilize the LLM generate method for speed gains.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/langchain/chains/llm.py:62\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m prompts, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_prompts(input_list)\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/langchain/llms/base.py:107\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m, prompts: List[PromptValue], stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    105\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    106\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/langchain/llms/base.py:140\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_end(output, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/langchain/llms/base.py:137\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    134\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m}, prompts, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/langchain/llms/base.py:324\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    322\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m--> 324\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
                        "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mCustomLLM._call\u001b[0;34m(self, prompt, stop)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    164\u001b[0m     prompt_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompt)\n\u001b[0;32m--> 165\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop_sequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[prompt_len:]\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:209\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/transformers/pipelines/base.py:1109\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1103\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m         )\n\u001b[1;32m   1107\u001b[0m     )\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/transformers/pipelines/base.py:1116\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1115\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1116\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1117\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/transformers/pipelines/base.py:1015\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1014\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1015\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:251\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m prompt_text \u001b[38;5;241m=\u001b[39m model_inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_text\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/peft/peft_model.py:580\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config, PromptLearningConfig):\n\u001b[0;32m--> 580\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/transformers/generation/utils.py:1437\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1432\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences has to be 1, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when doing\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1433\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m greedy search.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1434\u001b[0m         )\n\u001b[1;32m   1436\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1442\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1443\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1445\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1447\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/transformers/generation/utils.py:2248\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2245\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2247\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2248\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2251\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2252\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2256\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:687\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    686\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 687\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    700\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:577\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    569\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    570\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    571\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:305\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    303\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    304\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 305\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    308\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:157\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
                        "File \u001b[0;32m/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": "import logging\ntemplate = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\n{instruction}\nAnswer:\"\"\"\n\n\nfrom langchain import PromptTemplate, LLMChain\nprompt = PromptTemplate(template=template, input_variables=[\"instruction\"])\n\nllm_chain = LLMChain(prompt=prompt,\n                     llm=local_llm)\n\n\ndef evaluate(\n    instruction\n):\n    instruction = template.replace(\"{instruction}\",instruction)\n    #return index.query(instruction)\n    #return agend.run(instruction)\n    logging.info(\"___________Promt:____________\");\n    logging.info(instruction);\n    logging.info(\"_________Normal model____________\");\n    logging.info(llm_chain.run(instruction));\n    logging.info(\"_________After Documents___________\");\n    logging.info(index.query(instruction));\n    logging.info(\"______________________________\");\n\n\nevaluate(\"What do you think of Facebook's LLaMa?\")\nevaluate(\"How many people lives in Martos?\")\nevaluate(\"What is the capital of England?\")\nevaluate(\"What are alpacas? and how are they different from llamas?\")\nevaluate(\"how much is 213769*121239?\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# pip install -q transformers accelerate\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigscience/bloomz-7b1-mt\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=\"auto\", device_map=\"auto\")\n\ninputs = tokenizer.encode(\"Translate the sentence 'I have no mouth but I must scream' into Spanish\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: langchain in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (0.0.132)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from langchain) (8.2.2)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from langchain) (2.28.1)\nRequirement already satisfied: PyYAML>=5.4.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from langchain) (6.0)\nRequirement already satisfied: pydantic<2,>=1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from langchain) (1.10.7)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from langchain) (3.8.4)\nRequirement already satisfied: SQLAlchemy<2,>=1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from langchain) (1.4.39)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from langchain) (1.23.1)\nRequirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from langchain) (0.5.7)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (5.2.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.4.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\nRequirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\nRequirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from pydantic<2,>=1->langchain) (4.3.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.11)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2022.12.7)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from SQLAlchemy<2,>=1->langchain) (1.1.1)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (21.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (3.0.9)\nRequirement already satisfied: llama_index in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (0.5.8)\nRequirement already satisfied: numpy in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from llama_index) (1.23.1)\nRequirement already satisfied: langchain in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from llama_index) (0.0.132)\nRequirement already satisfied: tenacity<9.0.0,>=8.2.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from llama_index) (8.2.2)\nRequirement already satisfied: openai>=0.26.4 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from llama_index) (0.27.4)\nRequirement already satisfied: tiktoken in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from llama_index) (0.3.3)\nRequirement already satisfied: dataclasses-json in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from llama_index) (0.5.7)\nRequirement already satisfied: pandas in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from llama_index) (1.4.3)\nRequirement already satisfied: aiohttp in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from openai>=0.26.4->llama_index) (3.8.4)\nRequirement already satisfied: tqdm in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from openai>=0.26.4->llama_index) (4.64.0)\nRequirement already satisfied: requests>=2.20 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from openai>=0.26.4->llama_index) (2.28.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from dataclasses-json->llama_index) (3.19.0)\nRequirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from dataclasses-json->llama_index) (1.5.1)\nRequirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from dataclasses-json->llama_index) (0.8.0)\nRequirement already satisfied: pydantic<2,>=1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from langchain->llama_index) (1.10.7)\nRequirement already satisfied: PyYAML>=5.4.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from langchain->llama_index) (6.0)\nRequirement already satisfied: SQLAlchemy<2,>=1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from langchain->llama_index) (1.4.39)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from pandas->llama_index) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from pandas->llama_index) (2022.1)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from tiktoken->llama_index) (2022.7.9)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->openai>=0.26.4->llama_index) (1.8.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->openai>=0.26.4->llama_index) (1.2.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->openai>=0.26.4->llama_index) (4.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->openai>=0.26.4->llama_index) (21.4.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->openai>=0.26.4->llama_index) (2.0.4)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->openai>=0.26.4->llama_index) (5.2.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from aiohttp->openai>=0.26.4->llama_index) (1.2.0)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json->llama_index) (21.3)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from pydantic<2,>=1->langchain->llama_index) (4.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->llama_index) (1.16.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests>=2.20->openai>=0.26.4->llama_index) (1.26.11)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests>=2.20->openai>=0.26.4->llama_index) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests>=2.20->openai>=0.26.4->llama_index) (2022.12.7)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from SQLAlchemy<2,>=1->langchain->llama_index) (1.1.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json->llama_index) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json->llama_index) (3.0.9)\nRequirement already satisfied: sentence_transformers in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from sentence_transformers) (0.13.3)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from sentence_transformers) (2.0.0)\nRequirement already satisfied: numpy in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from sentence_transformers) (1.23.1)\nRequirement already satisfied: sentencepiece in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from sentence_transformers) (0.1.96)\nRequirement already satisfied: scipy in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from sentence_transformers) (1.8.1)\nRequirement already satisfied: tqdm in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from sentence_transformers) (4.64.0)\nRequirement already satisfied: torchvision in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from sentence_transformers) (0.13.1)\nRequirement already satisfied: scikit-learn in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from sentence_transformers) (1.1.1)\nRequirement already satisfied: nltk in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from sentence_transformers) (3.8.1)\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from sentence_transformers) (4.28.0.dev0)\nRequirement already satisfied: filelock in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\nRequirement already satisfied: requests in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.3.0)\nRequirement already satisfied: networkx in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (2.8.4)\nRequirement already satisfied: jinja2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.0.3)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.4.0.1)\nRequirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.91)\nRequirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (2.14.3)\nRequirement already satisfied: sympy in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (10.9.0.58)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.10.3.66)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.99)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.4.91)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.101)\nRequirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (10.2.10.91)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.99)\nRequirement already satisfied: triton==2.0.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (8.5.0.96)\nRequirement already satisfied: setuptools in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence_transformers) (65.6.3)\nRequirement already satisfied: wheel in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence_transformers) (0.38.4)\nRequirement already satisfied: lit in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.0)\nRequirement already satisfied: cmake in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.26.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.11.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.7.9)\nRequirement already satisfied: joblib in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.1.1)\nRequirement already satisfied: click in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from nltk->sentence_transformers) (8.0.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (2.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torchvision->sentence_transformers) (9.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.11)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\nRequirement already satisfied: PyPDF2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: google-api-python-client in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (2.84.0)\nRequirement already satisfied: google-auth-httplib2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (0.1.0)\nRequirement already satisfied: google-auth-oauthlib in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (0.4.4)\nCollecting google-auth-oauthlib\n  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from google-api-python-client) (4.1.1)\nRequirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from google-api-python-client) (0.22.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from google-api-python-client) (2.11.0)\nRequirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from google-api-python-client) (2.17.2)\nRequirement already satisfied: six in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from google-auth-httplib2) (1.16.0)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from google-auth-oauthlib) (1.3.0)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (1.56.4)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (3.19.6)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2.28.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (0.2.8)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (4.2.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (4.7.2)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client) (3.0.9)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (0.4.8)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (1.26.11)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2022.12.7)\nInstalling collected packages: google-auth-oauthlib\n  Attempting uninstall: google-auth-oauthlib\n    Found existing installation: google-auth-oauthlib 0.4.4\n    Uninstalling google-auth-oauthlib-0.4.4:\n      Successfully uninstalled google-auth-oauthlib-0.4.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorboard 2.9.1 requires google-auth-oauthlib<0.5,>=0.4.1, but you have google-auth-oauthlib 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed google-auth-oauthlib-1.0.0\n"
                }
            ],
            "source": "!pip -q install datasets loralib sentencepiece\n!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n!pip -q install git+https://github.com/huggingface/peft.git\n!pip -q install bitsandbytes\n \n!pip install langchain\n!pip install llama_index\n!pip install sentence_transformers\n \n!pip install PyPDF2  #if you want to load PDF\n!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Overwriting credentials.json\n"
                }
            ],
            "source": "%%writefile  credentials.json\n{\"installed\":{\"client_id\":\"1076209596528-hpbgjtsq88fr4sv7g60ogd2377okmtpq.apps.googleusercontent.com\",\"project_id\":\"acquired-voice-353914\",\"auth_uri\":\"https://accounts.google.com/o/oauth2/auth\",\"token_uri\":\"https://oauth2.googleapis.com/token\",\"auth_provider_x509_cert_url\":\"https://www.googleapis.com/oauth2/v1/certs\",\"client_secret\":\"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\",\"redirect_uris\":[\"http://localhost\"]}}"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Overwriting client_secrets.json\n"
                }
            ],
            "source": "%%writefile client_secrets.json\n{\"installed\":{\"client_id\":\"1076209596528-hpbgjtsq88fr4sv7g60ogd2377okmtpq.apps.googleusercontent.com\",\"project_id\":\"acquired-voice-353914\",\"auth_uri\":\"https://accounts.google.com/o/oauth2/auth\",\"token_uri\":\"https://oauth2.googleapis.com/token\",\"auth_provider_x509_cert_url\":\"https://www.googleapis.com/oauth2/v1/certs\",\"client_secret\":\"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\",\"redirect_uris\":[\"http://localhost\"]}}"
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9ffccaa82d8e4242bb8dc8024bb3a4c2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\nINFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\nINFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f6e1deb6c29b496b9fe0199d0e807715",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3e6baa52475841bd9053b02a7359a20e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "00f9fe7db99445e48efb04e448793d26",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\nINFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 277 tokens\n"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4805160c688344f99fb33a0086c6c57f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 128 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 6 tokens\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model Response: nse: This document provides instructions for troubleshooting a display issue on an aircraft.\nnse: This document provides instructions for troubleshooting a display issue on an aircraft.\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "UsageError: Line magic function `%%time` not found.\n"
                }
            ],
            "source": "import torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\nimport textwrap\nimport os\nfrom llama_index import download_loader\nfrom langchain.llms.base import LLM\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\nfrom llama_index import LangchainEmbedding\nfrom llama_index import SimpleDirectoryReader, LLMPredictor, GPTSimpleVectorIndex, ServiceContext, PromptHelper\nfrom peft import PeftModel\nfrom transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n#Load GPT4ALL-LORA Model\n \npeft_model_id = \"nomic-ai/gpt4all-lora\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\n \nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, \n                                             return_dict=True, \n                                             device_map='auto')\n \ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n \n# Load the Lora model\nmodel = PeftModel.from_pretrained(model, peft_model_id)\n \ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\nmax_length = 1500 #2048\nmax_new_tokens = 48\n\n \n# Load PDF from data directory and ask questions.\n# Larger PDFs crash the colab.\n \nclass LLaMALLM(LLM):\n    def _call(self, prompt, stop=None):\n        prompt += \"### Response:\"\n \n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n        input_ids = inputs[\"input_ids\"].to('cpu')\n        \n        generation_config = GenerationConfig(\n            temperature=0.6,\n            top_p=0.95,\n            repetition_penalty=1.15,\n        )\n        with torch.no_grad():\n            generation_output = model.generate(\n                input_ids=input_ids,\n                generation_config=generation_config,\n                return_dict_in_generate=True,\n                output_scores=True,\n                max_new_tokens=128,\n            )\n        response = \"\"\n        for s in generation_output.sequences:\n            response += tokenizer.decode(s)\n            \n        response = response[len(prompt):]\n        print(\"Model Response:\", response)\n        return response\n \n    def _identifying_params(self):\n        return {\"name_of_model\": \"alpaca\"}\n \n    def _llm_type(self):\n        return \"custom\"\n#only needed for pompt helper - which i turned off.\nmax_input_size = max_length\nnum_output = max_new_tokens\nmax_chunk_overlap = 20\n \nprompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\nembed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n \n#data is where your documents need to go\n\n\nPptxReader = download_loader(\"PptxReader\")\n\nloader = PptxReader()\n\n# Set the directory path\ndirectory_path = './pptx'\ndocuments = []\n# Iterate over each file in the directory\nfor filename in os.listdir(directory_path):\n    # Print the filename\n    doc = loader.load_data(file=directory_path+'/'+filename)\n    documents+=doc\n\n\nllm_predictor = LLMPredictor(llm=LLaMALLM())\n#index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor, embed_model=embed_model, prompt_helper=prompt_helper)\n \nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model)\nindex = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)\n \nindex.save_to_disk('index.json')\nnew_service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model)\nnew_index = GPTSimpleVectorIndex.load_from_disk('index.json', service_context=new_service_context)\n \nresponse = new_index.query(\"What is this document about?\")\nprint(response.response)\n \n\ndef gpt4all_generate(text):\n    inputs = tokenizer(\n        text,\n        return_tensors=\"pt\",\n    )\n    input_ids = inputs[\"input_ids\"].to('cpu')\n \n    generation_config = GenerationConfig(\n        temperature=0.6,\n        top_p=0.95,\n        repetition_penalty=1.2,\n    )\n \n    print(\"Generating...\")\n    generation_output = model.generate(\n        input_ids=input_ids,\n        generation_config=generation_config,\n        # return_dict_in_generate=True,\n        output_scores=True,\n        max_new_tokens=256,\n    )\n \n    wrapped_text = textwrap.fill(tokenizer.decode(generation_output[0]), width=100)\n    print(wrapped_text)\n \n \n%%time\nPROMPT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\nWhat is the differences between a cigarette and a cigar?\n### Response:\"\"\"\n \ngpt4all_generate(PROMPT)\n \nPROMPT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\nExplain what a rainbow is.\n### Response:\"\"\"\ngpt4all_generate(PROMPT)\n "
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": "import ibm_boto3\nfrom ibm_botocore.client import Config\n\n# Set up the client\ncos_client = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='w9F6fh9Ka-SJqIxxnUfhd4Zcxsl6k9NWHzWqZbpal9Sh',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.private.us-south.cloud-object-storage.appdomain.cloud')\n\nbucket = 'llmtesting-donotdelete-pr-srjop7o7om5peh'\n\n\n# Retrieve a list of object keys in the bucket\nobject_list = cos_client.list_objects(Bucket=bucket)['Contents']\n\n# Download each file in the object list\nfor object in object_list:\n    file_name = object['Key']\n    if 'pptx' in file_name:\n        cos_client.download_file(bucket, file_name,file_name)"
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "26ee55e579e142e5bcc810b42afecda4",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 177 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 3 tokens\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model Response: nse: Yes, it seems like there might be an issue with the display on this particular device. The troubleshooting steps mentioned in the SOP suggest that a quick fix may involve checking for loose connections or replacing faulty parts. However, if the problem persists after these initial repairs are made, then further investigation and troubleshooting may be necessary.\nnse: Yes, it seems like there might be an issue with the display on this particular device. The troubleshooting steps mentioned in the SOP suggest that a quick fix may involve checking for loose connections or replacing faulty parts. However, if the problem persists after these initial repairs are made, then further investigation and troubleshooting may be necessary.\n"
                }
            ],
            "source": "response = new_index.query(\"can't display\")\nprint(response.response)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10 + GPU",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}